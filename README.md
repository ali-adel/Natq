# ğŸ—£ï¸ Natq: Arabic Text-to-Speech System

![License](https://img.shields.io/badge/license-MIT-green.svg)
![Python](https://img.shields.io/badge/python-3.10+-blue.svg)

**Natq** (Ù†Ø·Ù‚) is an open-source Arabic Text-to-Speech (TTS) system developed as a graduation project at the Faculty of Computing and Artificial Intelligence, Helwan University. This system supports high-quality Arabic speech synthesis using modern deep learning modelsâ€”built from scratch using public datasets and designed to handle diacritization and grapheme-to-phoneme (G2P) conversion for Arabic.

<p align="center">
  <img src="assets/natq_pipeline.png" alt="Pipeline" width="80%">
</p>

---

## ğŸŒŸ Highlights

- ğŸ™ï¸ Arabic support with diacritics & phoneme-level control
- âš™ï¸ Models: FastPitch, FastSpeech2, Mixer-TTS, Spark-TTS, HiFi-GAN
- ğŸ¤– End-to-end & modular pipelines
- ğŸ”ˆ High-quality waveform synthesis
- ğŸš€ Deployment-ready with FastAPI & React
- ğŸ—ƒï¸ Open datasets: ClArTTS & Arabic Speech Corpus

---

## ğŸ“œ Abstract

Arabic is a complex and low-resource language for TTS. Our project tackles the challenges of limited annotated speech corpora and the absence of diacritics, which are essential for correct pronunciation. We propose a modular Arabic TTS system using deep learning with:

- Transformer-based mel-spectrogram generators
- A state-of-the-art HiFi-GAN vocoder
- A novel LLM-based TTS model: Spark-TTS
- Integration of diacritization (CATT) and phoneme conversion (Nawar Halabiâ€™s Phenomizer)

---

## ğŸ§  Model Architecture

<p align="center">
  <img src="assets/models_overview.png" alt="Models" width="80%">
</p>

### ğŸ”§ Spectrogram Generators
- [FastPitch](https://arxiv.org/abs/2006.06873)
- [FastSpeech2](https://arxiv.org/abs/2006.04558)
- [Mixer-TTS](https://arxiv.org/abs/2306.12313)
- [Spark-TTS](https://arxiv.org/abs/2401.05930)

### ğŸŒ€ Vocoder
- [HiFi-GAN](https://arxiv.org/abs/2010.05646)

### ğŸ”¡ Preprocessing
- CATT for diacritization
- Nawar Halabi's G2P converter (Phenomizer)

---

## ğŸ§ª Samples

| Model        | Sample 1 | Sample 2 |
|--------------|----------|----------|
| FastPitch    | ğŸ”Š [Play](samples/fastpitch_1.wav) | ğŸ”Š [Play](samples/fastpitch_2.wav) |
| Mixer-TTS    | ğŸ”Š [Play](samples/mixer_1.wav)    | ğŸ”Š [Play](samples/mixer_2.wav)    |
| Spark-TTS    | ğŸ”Š [Play](samples/spark_1.wav)    | ğŸ”Š [Play](samples/spark_2.wav)    |
| FastSpeech2  | ğŸ”Š [Play](samples/fs2_1.wav)      | ğŸ”Š [Play](samples/fs2_2.wav)      |

> ğŸ§ All models evaluated on both ClArTTS and Arabic Speech Corpus.

---

## ğŸš€ Deployment

You can deploy the TTS system locally using:

### Backend (FastAPI)
```bash
cd app
pip install -r requirements.txt
uvicorn main:app --reload

